function create_assembly_sbt_file {
        ASSEMBLY_SBT_FILE=${GEN_APPSDIR}/scala/${APPLICATION}/project/assembly.sbt
        [ -f ${ASSEMBLY_SBT_FILE} ] && rm -f ${ASSEMBLY_SBT_FILE}
        cat >> $ASSEMBLY_SBT_FILE << !
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.6")
//addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")
!
}
#
function create_build_sbt_file {
        BUILD_SBT_FILE=${GEN_APPSDIR}/scala/${APPLICATION}/build.sbt
        [ -f ${BUILD_SBT_FILE} ] && rm -f ${BUILD_SBT_FILE}
        cat >> $BUILD_SBT_FILE << !
lazy val root = (project in file(".")).
  settings(
    name := "${APPLICATION}",
    version := "1.0",
    scalaVersion := "2.11.8",
    mainClass in Compile := Some("myPackage.${APPLICATION}")
  )


libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.4.0"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.0.0"  % "provided" exclude("org.apache.hadoop", "hadoop-client")
resolvers += "Akka Repository" at "http://repo.akka.io/releases/"
resolvers += "Hortonworks Repository" at "http://repo.hortonworks.com/content/repositories/releases/"
libraryDependencies += "com.amazonaws" % "aws-java-sdk" % "1.7.8"
libraryDependencies += "commons-io" % "commons-io" % "2.4"
libraryDependencies += "javax.servlet" % "javax.servlet-api" % "3.0.1" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.0.0" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-hive" % "2.0.0" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-streaming" % "2.0.0" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka" % "1.6.1" % "provided"
libraryDependencies += "org.apache.phoenix" % "phoenix-spark" % "4.6.0-HBase-1.0"
libraryDependencies += "org.apache.hbase" % "hbase" % "1.2.3"
libraryDependencies += "org.apache.hbase" % "hbase-client" % "1.2.6"
#libraryDependencies += "org.apache.hbase" % "hbase-common" % "1.2.6"
libraryDependencies += "org.apache.hbase" % "hbase-server" % "1.2.6"
libraryDependencies += "org.mongodb.spark" %% "mongo-spark-connector" % "2.2.0"
libraryDependencies += "org.mongodb" % "mongo-java-driver" % "3.8.1"
libraryDependencies += "org.apache.spark" %% "spark-streaming-twitter" % "1.6.3"
libraryDependencies += "com.google.cloud.bigdataoss" % "bigquery-connector" % "0.13.4-hadoop3"
libraryDependencies += "com.google.cloud.bigdataoss" % "gcs-connector" % "1.9.4-hadoop3"
libraryDependencies += "com.google.code.gson" % "gson" % "2.8.5"
libraryDependencies += "com.google.guava" % "guava" % "27.0.1-jre"
libraryDependencies += "org.apache.httpcomponents" % "httpcore" % "4.4.8"
libraryDependencies += "com.hortonworks" % "shc-core" % "1.1.1-2.1-s_2.11"

// META-INF discarding

assemblyMergeStrategy in assembly := {
 case PathList("META-INF", xs @ _*) => MergeStrategy.discard
 case x => MergeStrategy.first
}
!
}
#
function default_settings {
export PACKAGES="com.databricks:spark-csv_2.11:1.3.0"
export SCHEDULER="spark.scheduler.mode=FIFO"
export EXTRAJAVAOPTIONS="spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
export JARS="/home/hduser/jars/spark-streaming-kafka-assembly_2.11-1.6.1.jar"
export SPARKUI="spark.ui.port="
export SPARKDRIVERPORT="spark.driver.port=54631"
export SPARKFILESERVERPORT="spark.fileserver.port=54731"
export SPARKBLOCKMANAGERPORT="spark.blockManager.port=54832"
export SPARKKRYOSERIALIZERBUFFERMAX="spark.kryoserializer.buffer.max=512"
}

function run_yarn {
default_settings
if [[ -z ${SP} ]]
then
        SPARKUIPORT="${SPARKU}55555"
else
        SPARKUIPORT="${SPARKUI}${SP}"
fi

${SPARK_HOME}/bin/spark-submit \
                --packages ${PACKAGES} \
                --executor-memory 6G \
                --num-executors 12 \
                --executor-cores 4 \
                --master yarn \
                --deploy-mode client \
                --conf "${SCHEDULER}" \
                --conf "${EXTRAJAVAOPTIONS}" \
                --jars ${JARS} \
                --class "${FILE_NAME}" \
                --conf "${SPARKUIPORT}" \
                --conf "${SPARKDRIVERPORT}" \
                --conf "${SPARKFILESERVERPORT}" \
                --conf "${SPARKBLOCKMANAGERPORT}" \
                --conf "${SPARKKRYOSERIALIZERBUFFERMAX}" \
                --conf spark.yarn.executor.memoryOverhead=3000 \
                ${JAR_FILE}
}
#

